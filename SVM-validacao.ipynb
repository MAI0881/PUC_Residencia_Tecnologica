{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f17c237",
   "metadata": {},
   "source": [
    "# 1 Normalização e Padronização"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b25cfd3",
   "metadata": {},
   "source": [
    "Normalização e padronização são processos usados para preparar dados antes de apicar modelos de ML ou estatístico. O objetivo das técnicas é melhora de desempenho e garantir que diferentes variáveis estejam em escalas comparáveis. No entanto, são duas abordagens diferentes para situações diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55be69c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dados = np.random.randint(10, 100, size=(100,5))\n",
    "df = pd.DataFrame(dados)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9ae60e",
   "metadata": {},
   "source": [
    "## Normalização Min-Max\n",
    "É o processo de ajustar variáveis para que fiquem dentro de um intervalo específico. Normalmente 0 e 1 na técnica Min-Max Scaling pois temos o valor máximo sendo 1 e o valor mínimo sendo 0 tornando mais fácil comparar e interpretar as relações entre as variáveis por conta da ausência de valores discrepantes.\n",
    "Algoritmos como regressão linear e redes neurais são sensíveis a valores discrepantes e por isso normalizar para a faixa 0-1 ajuda a evitar que recursos com valores muito maiores afete desproporcionalmente o resultado do modelo.\n",
    "Além dos coeficientes do modelo linear terem uma interpretação melhor com a escala normalizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abfb932",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dfcb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = (df - df.min()) / (df.max() - df.min())\n",
    "norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f19119",
   "metadata": {},
   "source": [
    "## Padronização z-score\n",
    "\n",
    "Essa técnica visa colocar todos os valores em uma escala onde a média seja 0 e o desvio padrão seja 1 pois em muitos casos de modelagem para algoritmos de aprendizado de máquina não estamos interessados na média absoluta dos dados e sim nas diferenças dos dados em relação a média. Centralizar os valores em torno da média facilita a interpretação e análise dos dados.\n",
    "Os dados tendo DP igual a 1 também temos uma escala comparável para algortimos sensíveis as escalas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a9a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "padr = (df - df.mean()) / df.std()\n",
    "padr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb584cb",
   "metadata": {},
   "source": [
    "## Underfitting e Overfitting\n",
    "\n",
    "O subajuste é comum quando o modelo é incapaz de se ajustar adequadamente aos dados de treinamento, ou seja, o modelo não consegue capturar as relações subjacentes nos dados resultando em um desempenho insatisfatório.\n",
    "\n",
    "* Isso pode acontecer porque o modelo não consegue reduzir os erros nos dados de treinamento, logo, não consegue aprender com os dados efetivamente;\n",
    "* O subajuste ocorre muito quando o modelo é simples demais em relação a complexidade dos dados como uma regressão linear emq ue os dados não possuem uma relação linear;\n",
    "* Como não se ajustou bem aos dados de treinamento, o modelo é incapaz de generalizar para novos dados;\n",
    "* Um modelo com viés elevado pode fazer previsões simplistas  sendo capaz de generalizar depois para novos dados\n",
    "\n",
    "Enquanto o sobreajuste é um fenômeno que ocorre quando o modelo se ajusta muito aos dados de treino e se torna incapax de generalizar os resultados para dados não aprendidos, como os dados de teste por exemplo.\n",
    "\n",
    "* Pode ocorrer quando temos modelos muito complexos, ou seja com muitos parâmetros ou graus de liberdade como redes neurais profundas ou árvores de decisão profundas;\n",
    "* Quando treinamos um modelo muito complexo com poucos dados impossibilitando que o modelo generalize efetivamente;\n",
    "* Dados com ruídos e aleatoriedades podem impedir que o modelo capture relações genuínas e acabe aprendendo a ajustar esses ruídos;\n",
    "* Utilizar um número excessivo de épocas ou iterações ao treinar o modelo. Não parar o treinamento no momento correto;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb234b",
   "metadata": {},
   "source": [
    "## Learning rate\n",
    "\n",
    "O nível de aprendizado é o parâmetro que define o ajuste dos pesos da nossa rede em respeito ao gradiente descendente de custo. Ele determina o quão rápido o algoritmo se ajusta em direção ao peso ótimo.\n",
    "Se esssa taxa for muito alta,  nós vamos pular a solução ótima e se for muita baixa precisaremos de muitas iterações para convergir para os melhores valores.\n",
    "Em outras palavras, o Learning Rate é o quão rápido nossa rede abandona conceitos que ela aprendeu até agora para novos conceitos serem aprendidos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138874c0",
   "metadata": {},
   "source": [
    "## Modelos de otimização\n",
    "\n",
    "Função objetivo: É o que será maximizado ou minimzado. Expressa o objetivo do problema em termos das variáveis de decisão. Costuma ser uma combinação matemática das variáveis de decisão\n",
    "\n",
    "Restrições: São as regras que limitam as possíveis soluções do problema. Podem ser equações ou desigualdades  que conectam as variáveis de decisão e devem ser atendidas para que a solução seja considerada válida.\n",
    "\n",
    "Variáveis de decisão: São as variáveis que serão ajustadas para otimzar o problema.\n",
    "\n",
    "O processo de formulação de um modelo matemático de um problema de otimização é crucial para que se possa aplicar técnicas de otimização, como programação linear, não linear, inteira etc para encontrar a solução ótima que atenda aos objetivos do problema, respeitando as restrições impostas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28797377",
   "metadata": {},
   "source": [
    "### Problema\n",
    "\n",
    "Suponha que você trabalhe em uma fábrica de móveis e tenha uma chapa de madeira de tamanho fixo, por exemplo, 4 metros por 2 metros, e você precisa cortá-la para produzir peças de móveis. Cada peça de móvel requer um tamanho específico de corte na chapa de madeira. O objetivo é maximizar a quantidade de peças de móveis que você pode produzir a partir da chapa de madeira, minimizando o desperdício de material.\n",
    "\n",
    "Aqui estão algumas informações adicionais:\n",
    "\n",
    "Você tem uma lista de peças de móveis que precisam ser cortadas a partir da chapa de madeira, cada uma com dimensões específicas.\n",
    "Cada corte na chapa de madeira resulta em uma perda de material devido à largura da serra ou à espessura da lâmina de corte.\n",
    "Suas peças de móveis não podem se sobrepor na chapa de madeira, e você não pode cortar partes de uma peça de móvel para encaixar em outra.\n",
    "O problema é encontrar o arranjo ótimo para cortar as peças de móveis na chapa de madeira de forma a maximizar o número de peças produzidas, minimizando o desperdício de material.\n",
    "\n",
    "Você pode formular esse problema como um problema de programação linear ou um problema de programação inteira e usar algoritmos de otimização para encontrar a solução ótima.\n",
    "\n",
    "Este é um problema comum em indústrias de manufatura e pode ser abordado de várias maneiras, dependendo das restrições específicas e das características das peças de móveis em questão. É um ótimo exercício de otimização que requer habilidades de modelagem e resolução de problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e60f5c",
   "metadata": {},
   "source": [
    "### Regras\n",
    "Restrições:\n",
    "* Suas peças de móveis não podem se sobrepor na chapa de madeira\n",
    "* Você não pode cortar partes de uma peça de móvel para encaixar em outra.\n",
    "\n",
    "Objetivo:\n",
    "* Encontrar o arranjo ótimo para cortar as peças de móveis na chapa de madeira de forma a maximizar o número de peças produzidas.\n",
    "\n",
    "Variáveis de decisão:\n",
    "\n",
    "* xP Tamanho de cada peça (peça 1 =  1 x 0.5, peça 2 = 1 x 0.3, peça 3 = 1.5 x 0.7) = 3.5 x 1.5 = 0.5 x 0.5\n",
    "* xC Tamanho da chapa que será cortada ( 4 x 2 )\n",
    "* xD Desperdício final ( 0.5 x 0.5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8a6d9f",
   "metadata": {},
   "source": [
    "xP = 3.5 x 1.5 = \n",
    "xC = 4 X 2 =\n",
    "xD <= 0.5 x 0.5 = \n",
    "\n",
    "max = 3.5 x 1.5*xP + 4 X 2*xC + 0.5 x 0.5*xD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29691691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pulp in c:\\users\\maiara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.7.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pulp\n",
    "from pulp import *\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Criação do problema\n",
    "prob = LpProblem(\"Produção de peças\", LpMaximize)\n",
    "\n",
    "# Variáveis de decisão\n",
    "xP = LpVariable(\"xP\", 0, None)\n",
    "xC = LpVariable(\"xC\", 0, None)\n",
    "xD = LpVariable(\"xD\", 0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9223b8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função objetivo\n",
    "prob += 5.25*xP + 8*xC + 0.25*xD, \"Retorno Esperado\"\n",
    "\n",
    "# Restrições\n",
    "prob += xA + xT + xFI <= 1000, \"Capital Disponível\"\n",
    "prob += xA <= 0.5*1000, \"Restrição de Ações\"\n",
    "prob += xFI <= 0.3*1000, \"Restrição de Fundos Imobiliários\"\n",
    "prob += xT >= 0.1*1000, \"Restrição de Títulos\"\n",
    "\n",
    "# Resolução do problema\n",
    "prob.solve()\n",
    "\n",
    "# Impressão da solução\n",
    "print(\"Status da solução:\", LpStatus[prob.status])\n",
    "print(\"Valor ótimo da função objetivo:\", value(prob.objective))\n",
    "print(\"xA:\", value(xA.varValue))\n",
    "print(\"xT:\", value(xT.varValue))\n",
    "print(\"xFI:\", value(xFI.varValue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20af197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfd43808",
   "metadata": {},
   "source": [
    "# 2 Conteúdo adicional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4973ad5e",
   "metadata": {},
   "source": [
    "### Validação cruzada\n",
    "\n",
    "Técnica onde se divide a base de dados em k partes iguais permitindo que o modelo seja testado várias vezes em diferentes partições dos dados\n",
    "\n",
    "* Divisão dos Dados: \n",
    "O conjunto de dados é dividido em k partes iguais, chamadas de \"fold\" ou dobras. Por exemplo, se k for 5, o conjunto de dados será dividido em 5 partes iguais.\n",
    "\n",
    "* Treinamento e Teste: \n",
    "O modelo é treinado em k-1 das partes (folds) e testado na parte restante. Isso é repetido k vezes, com cada uma das k partes sendo usada como conjunto de teste uma vez, enquanto as outras k-1 partes são usadas como conjunto de treinamento.\n",
    "\n",
    "* Métricas de Desempenho: \n",
    "Para cada execução, calcula-se uma métrica de desempenho (como erro médio quadrático, acurácia, F1-score, etc.) no conjunto de teste. Essas métricas são usadas para avaliar o desempenho do modelo.\n",
    "\n",
    "* Média das Métricas: \n",
    "As métricas de desempenho calculadas em cada execução (fold) são então geralmente combinadas para produzir uma única medida de desempenho do modelo. Por exemplo, pode-se calcular a média das métricas de erro quadrático médio ou acurácia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a30083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04912dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = sns.load_dataset(\"iris\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f86e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2.drop(['species'], axis = 1)\n",
    "y = df2['species']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0564cf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a7e980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#By default, cv parameters assumes splits for StratifiedK-fold\n",
    "cv_scores_5_folds = cross_val_score(model, X, y, cv=5)\n",
    "#Specify K-Fold in cv parameter\n",
    "#cv_scores_5_folds = cross_val_score(model, X, y, cv=KFold(n_splits=5))\n",
    "\n",
    "cv_predicts_5_folds = cross_val_predict(model,X,y,cv=5)\n",
    "\n",
    "print(\"Accuracy score in each iteration: {}\".format(cv_scores_5_folds))\n",
    "print(\"Predicted class for each record: {}\".format(cv_predicts_5_folds))\n",
    "print(\"K-Fold Score: {}\".format(np.mean(cv_scores_5_folds)))\n",
    "print(\"Total records: {}, Total predicted values: {}\".format(df2.shape[0],len(cv_predicts_5_folds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae68e3c",
   "metadata": {},
   "source": [
    "### Exemplo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20778e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sns.load_dataset(\"titanic\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8539f1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64323028",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = data[['class','embark_town','survived']]\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7648914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "df3['class_encoded'] = label_encoder.fit_transform(df3['class'])\n",
    "df3['embark_town_encoded'] = label_encoder.fit_transform(df3['embark_town'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce71346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.drop(['class','embark_town'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf4f851",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e217343",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df3.drop(['survived'], axis = 1)\n",
    "y = df3['survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8624262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#By default, cv parameters assumes splits for StratifiedK-fold\n",
    "cv_scores_5_folds = cross_val_score(model, X, y, cv=10)\n",
    "#Specify K-Fold in cv parameter\n",
    "#cv_scores_5_folds = cross_val_score(model, X, y, cv=KFold(n_splits=5))\n",
    "\n",
    "cv_predicts_5_folds = cross_val_predict(model,X,y,cv=10)\n",
    "\n",
    "print(\"Accuracy score in each iteration: {}\".format(cv_scores_5_folds))\n",
    "print(\"Predicted class for each record: {}\".format(cv_predicts_5_folds))\n",
    "print(\"K-Fold Score: {}\".format(np.mean(cv_scores_5_folds)))\n",
    "print(\"Total records: {}, Total predicted values: {}\".format(df3.shape[0],len(cv_predicts_5_folds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6fe438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
